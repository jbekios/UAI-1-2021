{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"4-tranfer_learning.ipynb","provenance":[],"authorship_tag":"ABX9TyNNeWNFZOskiZuPsBWH+d30"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"l0-o6QYPLuLJ"},"source":["<center>\n","    <img src='images/small-uai.jpeg'style=\"width: 300px;\">\n","</center>\n","\n","## Laboratorio S04: _Deep Learning_ - _Transfer Learning_\n","\n","#### Curso: Aprendizaje profundo\n","   \n","<center>\n","    <img src='https://miro.medium.com/max/665/1*7Ip2_SeOz_BoruHEytEMlQ.png'style=\"width: 600px;\">\n","\n","    <sub><sup>https://www.viewnext.com/transfer-learning-y-redes-convolucionales/</sup></sub> \n","</center>\n","\n","**Profesor**: Dr. Juan Bekios Calfa\n","\n","**Grado**: MIA"]},{"cell_type":"markdown","metadata":{"id":"9qrfQIhBPTCS"},"source":["# Introducción\n","\n","Para esta implementación, usaremos el **VGG-16**. La red funciona bien para esta tarea de clasificación y es más rápido de entrenar que otros modelos. \n","\n","El proceso para utilizar un modelo previamente entrenado consta de los siguentes pasos:\n","\n","1.   Cargar pesos previamente entrenados de una red entrenada de un gran conjunto de datos.\n","2.   Congelar todos los pesos en las capas inferiores (convolucionales): las capas para congelar se ajustan según la similitud de la nueva tarea con el conjunto de datos original.\n","3.   Reemplazar las capas superiores de la red con un clasificador personalizado: el número de salidas debe establecerse igual al número de clases.\n","4.   Entrenar solo las capas de clasificador personalizado para la tarea, optimizando así el modelo para conjuntos de datos más pequeños\n","\n","# Cargar los datos"]},{"cell_type":"code","metadata":{"id":"KJV2NE39mahC","executionInfo":{"status":"ok","timestamp":1618260529236,"user_tz":240,"elapsed":722,"user":{"displayName":"Juan Bekios","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi2JWCO19gvDJyrxfHfNFNiXBEKcXQjNf-Ywq-j_w=s64","userId":"07279642735588932357"}}},"source":["import torch\n","import torchvision\n","from torchvision import transforms, datasets\n","import PIL"],"execution_count":42,"outputs":[]},{"cell_type":"code","metadata":{"id":"iVfTfsnsm0lX"},"source":["from google.colab import drive\n","drive.mount('/gdrive')\n","%cd /gdrive"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2Aa82mLJm1WV"},"source":["# Transformaciones sobre las imágenes\n","data_transform = transforms.Compose([\n","        transforms.Resize((224, 224)),\n","        transforms.RandomHorizontalFlip(),\n","        transforms.ToTensor(),\n","        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n","                             std=[0.229, 0.224, 0.225])\n","    ])\n","\n","# Carga de las imágenes\n","gatos_perros_train = datasets.ImageFolder(root='/gdrive/My Drive/D-UCN/Classes/TecnicasAvanzadasAprendizajeAutomatico/Laboratorios/Laboratorio05.2:DeepLearning/dataset/training_set',\n","                                           transform=data_transform)\n","gatos_perros_valid = datasets.ImageFolder(root='/gdrive/My Drive/D-UCN/Classes/TecnicasAvanzadasAprendizajeAutomatico/Laboratorios/Laboratorio05.2:DeepLearning/dataset/valid_set',\n","                                           transform=data_transform)\n","gatos_perros_test = datasets.ImageFolder(root='/gdrive/My Drive/D-UCN/Classes/TecnicasAvanzadasAprendizajeAutomatico/Laboratorios/Laboratorio05.2:DeepLearning/dataset/test_set',\n","                                           transform=data_transform)\n","\n","# Conjunto de entrenamiento\n","train_loader = torch.utils.data.DataLoader(gatos_perros_train,\n","                                             batch_size=32, shuffle=True,\n","                                             num_workers=2)\n","\n","# Conjunto de validación\n","valid_loader = torch.utils.data.DataLoader(gatos_perros_valid,\n","                                             batch_size=32, shuffle=False,\n","                                             num_workers=2)\n","\n","# Conjunto de pruebas\n","test_loader = torch.utils.data.DataLoader(gatos_perros_test,\n","                                             batch_size=32, shuffle=False,\n","                                             num_workers=2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"250XrS3tui03"},"source":["trainiter = iter(train_loader)\n","features, labels = next(trainiter)\n","features.shape, labels.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9MpXbvRfuxgI"},"source":["print('Hay ', len(gatos_perros_train.classes))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kD5e6_BAmb-4"},"source":["# 1. Cargar los pesos previamente entregados "]},{"cell_type":"code","metadata":{"id":"LzGnYHrqLvvn"},"source":["import torchvision.models as models\n","\n","model = models.vgg16(pretrained=True)\n","\n","model.classifier"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ThYQO8lpQVyL"},"source":["# 2. Congelar todos los pesos de las capas convolucionales"]},{"cell_type":"code","metadata":{"id":"Qp4olmeKMuAI"},"source":["# Congelar el modelo de pesos\n","for param in model.parameters():\n","    param.requires_grad = False"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZYo4yhfJQk6f"},"source":["# 3. Reemplazar las capas superiores de la red con un clasificador personalizado\n","\n","Agregamos nuestro propio clasificador personalizado con las siguientes capas:\n","\n","* Red neuronal totalmente conectada con activación de ReLU, forma = (n_inputs, 256)\n","* Drop out 40% de probabilidad\n","* Totalmente conectado con la salida de log softmax, shape = (256, n_classes)"]},{"cell_type":"code","metadata":{"id":"bOIGuS_rQlOG"},"source":["import torch.nn as nn\n","\n","n_inputs = 4096\n","n_classes = 2\n","# Agregar nuevo clasificador\n","model.classifier[6] = nn.Sequential(\n","                      nn.Linear(n_inputs, 256), \n","                      nn.ReLU(), \n","                      nn.Dropout(0.4),\n","                      nn.Linear(256, n_classes),                   \n","                      nn.LogSoftmax(dim=1))\n","\n","model"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xrdNA2mPRUFi"},"source":["Cuando las capas adicionales se agregan al modelo, se configuran como entrenables de forma predeterminada `(require_grad = True)`. Para la VGG-16, solo cambiaremos la última capa original completamente conectada. Todos los pesos en las capas convolucionales y las primeras 5 capas completamente conectadas no se pueden entrenar."]},{"cell_type":"code","metadata":{"id":"JyXANsffRUdP"},"source":["# Solo entrenar el clasificador[6]\n","model.classifier"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Jbq08zkFV_-7"},"source":["Calcular número de parámetros finales de la red"]},{"cell_type":"code","metadata":{"id":"KrWNzf8mWEip"},"source":["# Número total de parámetros y parámetros entrenables\n","total_params = sum(p.numel() for p in model.parameters())\n","print(f'{total_params:,} total de parámetros de la red.')\n","total_trainable_params = sum(\n","    p.numel() for p in model.parameters() if p.requires_grad)\n","print(f'{total_trainable_params:,} parametros entrenables.')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xl_Dicu1WnQS"},"source":["#4. Entrenar solo las capas de clasificador personalizado para la tarea\n","\n","##4.1 Mover el modelo a la GPU"]},{"cell_type":"code","metadata":{"id":"JwALBfjSWuVs"},"source":["device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","\n","model = model.to(device)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MAdtL9M4pZVP"},"source":["## 4.2 Optimización"]},{"cell_type":"code","metadata":{"id":"0I8-6ysnpdsc"},"source":["from torch import optim\n","# Definir función de perdida y optimización\n","criterion = nn.NLLLoss()\n","optimizer = optim.Adam(model.parameters())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BsgJsItvpjVN"},"source":["##4.3 "]},{"cell_type":"code","metadata":{"id":"RxY5xqRcpulQ"},"source":["def train(model,\n","          criterion,\n","          optimizer,\n","          train_loader,\n","          valid_loader,\n","          save_file_name,\n","          max_epochs_stop=3,\n","          n_epochs=20,\n","          print_every=2):\n","    \"\"\"Train a PyTorch Model\n","\n","    Params\n","    --------\n","        model (PyTorch model): cnn to train\n","        criterion (PyTorch loss): objective to minimize\n","        optimizer (PyTorch optimizier): optimizer to compute gradients of model parameters\n","        train_loader (PyTorch dataloader): training dataloader to iterate through\n","        valid_loader (PyTorch dataloader): validation dataloader used for early stopping\n","        save_file_name (str ending in '.pt'): file path to save the model state dict\n","        max_epochs_stop (int): maximum number of epochs with no improvement in validation loss for early stopping\n","        n_epochs (int): maximum number of training epochs\n","        print_every (int): frequency of epochs to print training stats\n","\n","    Returns\n","    --------\n","        model (PyTorch model): trained cnn with best weights\n","        history (DataFrame): history of train and validation loss and accuracy\n","    \"\"\"\n","\n","    # Early stopping intialization\n","    epochs_no_improve = 0\n","    valid_loss_min = np.Inf\n","\n","    valid_max_acc = 0\n","    history = []\n","\n","    # Number of epochs already trained (if using loaded in model weights)\n","    try:\n","        print(f'Model has been trained for: {model.epochs} epochs.\\n')\n","    except:\n","        model.epochs = 0\n","        print(f'Starting Training from Scratch.\\n')\n","\n","    overall_start = timer()\n","\n","    # Main loop\n","    for epoch in range(n_epochs):\n","\n","        # keep track of training and validation loss each epoch\n","        train_loss = 0.0\n","        valid_loss = 0.0\n","\n","        train_acc = 0\n","        valid_acc = 0\n","\n","        # Set to training\n","        model.train()\n","        start = timer()\n","\n","        # Training loop\n","        for ii, (data, target) in enumerate(train_loader):\n","            # Tensors to gpu\n","            if train_on_gpu:\n","                data, target = data.to(device), target.to(device)\n","            # Clear gradients\n","            optimizer.zero_grad()\n","            # Predicted outputs are log probabilities\n","            output = model(data)\n","\n","            # Loss and backpropagation of gradients\n","            loss = criterion(output, target)\n","            loss.backward()\n","\n","            # Update the parameters\n","            optimizer.step()\n","\n","            # Track train loss by multiplying average loss by number of examples in batch\n","            train_loss += loss.item() * data.size(0)\n","\n","            # Calculate accuracy by finding max log probability\n","            _, pred = torch.max(output, dim=1)\n","            correct_tensor = pred.eq(target.data.view_as(pred))\n","            # Need to convert correct tensor from int to float to average\n","            accuracy = torch.mean(correct_tensor.type(torch.FloatTensor))\n","            # Multiply average accuracy times the number of examples in batch\n","            train_acc += accuracy.item() * data.size(0)\n","\n","            # Track training progress\n","            print(\n","                f'Epoch: {epoch}\\t{100 * (ii + 1) / len(train_loader):.2f}% complete. {timer() - start:.2f} seconds elapsed in epoch.',\n","                end='\\r')\n","\n","        # After training loops ends, start validation\n","        else:\n","            model.epochs += 1\n","\n","            # Don't need to keep track of gradients\n","            with torch.no_grad():\n","                # Set to evaluation mode\n","                model.eval()\n","\n","                # Validation loop\n","                for data, target in valid_loader:\n","                    # Tensors to gpu\n","                    if train_on_gpu:\n","                        data, target = data.to(device), target.to(device)\n","\n","                    # Forward pass\n","                    output = model(data)\n","\n","                    # Validation loss\n","                    loss = criterion(output, target)\n","                    # Multiply average loss times the number of examples in batch\n","                    valid_loss += loss.item() * data.size(0)\n","\n","                    # Calculate validation accuracy\n","                    _, pred = torch.max(output, dim=1)\n","                    correct_tensor = pred.eq(target.data.view_as(pred))\n","                    accuracy = torch.mean(\n","                        correct_tensor.type(torch.FloatTensor))\n","                    # Multiply average accuracy times the number of examples\n","                    valid_acc += accuracy.item() * data.size(0)\n","\n","                # Calculate average losses\n","                train_loss = train_loss / len(train_loader.dataset)\n","                valid_loss = valid_loss / len(valid_loader.dataset)\n","\n","                # Calculate average accuracy\n","                train_acc = train_acc / len(train_loader.dataset)\n","                valid_acc = valid_acc / len(valid_loader.dataset)\n","\n","                history.append([train_loss, valid_loss, train_acc, valid_acc])\n","\n","                # Print training and validation results\n","                if (epoch + 1) % print_every == 0:\n","                    print(\n","                        f'\\nEpoch: {epoch} \\tTraining Loss: {train_loss:.4f} \\tValidation Loss: {valid_loss:.4f}'\n","                    )\n","                    print(\n","                        f'\\t\\tTraining Accuracy: {100 * train_acc:.2f}%\\t Validation Accuracy: {100 * valid_acc:.2f}%'\n","                    )\n","\n","                # Save the model if validation loss decreases\n","                if valid_loss < valid_loss_min:\n","                    # Save model\n","                    torch.save(model.state_dict(), save_file_name)\n","                    # Track improvement\n","                    epochs_no_improve = 0\n","                    valid_loss_min = valid_loss\n","                    valid_best_acc = valid_acc\n","                    best_epoch = epoch\n","\n","                # Otherwise increment count of epochs with no improvement\n","                else:\n","                    epochs_no_improve += 1\n","                    # Trigger early stopping\n","                    if epochs_no_improve >= max_epochs_stop:\n","                        print(\n","                            f'\\nEarly Stopping! Total epochs: {epoch}. Best epoch: {best_epoch} with loss: {valid_loss_min:.2f} and acc: {100 * valid_acc:.2f}%'\n","                        )\n","                        total_time = timer() - overall_start\n","                        print(\n","                            f'{total_time:.2f} total seconds elapsed. {total_time / (epoch+1):.2f} seconds per epoch.'\n","                        )\n","\n","                        # Load the best state dict\n","                        model.load_state_dict(torch.load(save_file_name))\n","                        # Attach the optimizer\n","                        model.optimizer = optimizer\n","\n","                        # Format history\n","                        history = pd.DataFrame(\n","                            history,\n","                            columns=[\n","                                'train_loss', 'valid_loss', 'train_acc',\n","                                'valid_acc'\n","                            ])\n","                        return model, history\n","\n","    # Attach the optimizer\n","    model.optimizer = optimizer\n","    # Record overall time and print out stats\n","    total_time = timer() - overall_start\n","    print(\n","        f'\\nBest epoch: {best_epoch} with loss: {valid_loss_min:.2f} and acc: {100 * valid_acc:.2f}%'\n","    )\n","    print(\n","        f'{total_time:.2f} total seconds elapsed. {total_time / (epoch):.2f} seconds per epoch.'\n","    )\n","    # Format history\n","    history = pd.DataFrame(\n","        history,\n","        columns=['train_loss', 'valid_loss', 'train_acc', 'valid_acc'])\n","    return model, history"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"or4vcZaurI6a"},"source":["### Corremos el entrenamiento"]},{"cell_type":"code","metadata":{"id":"FPVGMRpSrHkb"},"source":["import numpy as np\n","import pandas as pd\n","from timeit import default_timer as timer\n","\n","\n","model, history = train(\n","    model,\n","    criterion,\n","    optimizer,\n","    train_loader,\n","    valid_loader,\n","    save_file_name='model-1.model',\n","    max_epochs_stop=5,\n","    n_epochs=30,\n","    print_every=2)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pQQYWjA-pl2T"},"source":[""]},{"cell_type":"code","metadata":{"id":"Y0CRcVvHsAUL"},"source":["plt.figure(figsize=(8, 6))\n","for c in ['train_loss', 'valid_loss']:\n","    plt.plot(\n","        history[c], label=c)\n","plt.legend()\n","plt.xlabel('Epoch')\n","plt.ylabel('Average Negative Log Likelihood')\n","plt.title('Training and Validation Losses')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OmFUcIqXsBUH"},"source":[""]},{"cell_type":"code","metadata":{"id":"vmiPB1uSsCQS"},"source":["plt.figure(figsize=(8, 6))\n","for c in ['train_acc', 'valid_acc']:\n","    plt.plot(\n","        100 * history[c], label=c)\n","plt.legend()\n","plt.xlabel('Epoch')\n","plt.ylabel('Average Accuracy')\n","plt.title('Training and Validation Accuracy')"],"execution_count":null,"outputs":[]}]}