{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Redes-recurrentes-Alumnos.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N_kigFSYTVQ0"
      },
      "source": [
        "# Laboratorio 06: Redes Recurrentes\n",
        "\n",
        "**Objetivo:** Clasificar de apellidos, según su nacionalidad, utilizando un modelo simple de redes neuronales recurrentes. (A nivel de caracter) \n",
        "\n",
        "**Descripción:**\n",
        "\n",
        "Construiremos y entrenaremos un RNN de nivel básico para clasificar palabras. Este tutorial, junto con los dos siguientes, muestra cómo hacer el preprocesamiento de datos para el modelado de un proceso de lenguaje natural (NPL) \"desde cero\". Se utilizarán algunas de las funciones  de *torchtext*, de modo que se pueda ver cómo el preprocesamiento para el modelado de NPL funciona a bajo nivel.\n",
        "\n",
        "Una RNN a nivel de carácter lee las palabras como una serie de caracteres - produciendo una predicción y un \"estado oculto\" en cada paso, alimentando su estado oculto anterior en cada paso siguiente. Tomamos la predicción final como salida, es decir, a qué clase pertenece la palabra.\n",
        "\n",
        "Específicamente, entrenaremos en unos pocos miles de apellidos de 18 idiomas de origen, y predeciremos de qué idioma proviene un nombre basándonos en la ortografía.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "NLP From Scratch: Classifying Names with a Character-Level RNN\n",
        "\n",
        "**Autor:** Sean Robertson <https://github.com/spro/practical-pytorch>_\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LZSHMXpWTCI3"
      },
      "source": [
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rj1ql----tSA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7390ca5c-7695-44df-ad08-3cc69ed82e13"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Df5iZZvbRLOA"
      },
      "source": [
        "**Cargar los datos de Google Drive**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FkhUdtFdReN5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "a46a9f74-9896-4f74-d6ad-17d277f6a62b"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "%cd /gdrive"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n",
            "/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qVIx_MEvUxpJ"
      },
      "source": [
        "## 1. Preparando los \n",
        "Bajar la base de datos de apellidos `aquí <https://download.pytorch.org/tutorial/data.zip>`.\n",
        "\n",
        "Extraer estos en el directorio actual.\n",
        "\n",
        "En ``data/names`` se encuentran 18 textos en archivos llamados \"[lenguaje].txt\". Cada archivo contiene apellidos, uno por línea, la mayoría normalizados (pero deben ser convertidos de Unicode a ASCII).\n",
        "\n",
        "Terminaremos con un diccionario con listas de nombres por idioma,\n",
        "``Idioma: [nombres ...]}``. Las variables genéricas \"categoría\" y \"línea\"\n",
        "(para el lenguaje y el nombre en nuestro caso) se utilizan para su posterior ampliación."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9V5dOxa3YQq9"
      },
      "source": [
        "import unicodedata\n",
        "import string\n",
        "import glob\n",
        "import os\n",
        "\n",
        "PATH_TEXT = '/content/drive/My Drive/D-UCN/Classes/TecnicasAvanzadasAprendizajeAutomatico/Laboratorios/Laboratorio06:RedesRecurrentes/database/names'\n",
        "\n",
        "all_letters = string.ascii_letters + \" .,;'\"\n",
        "n_letters = len(all_letters)\n",
        "#print(n_letters)\n",
        "\n",
        "# Convertir el código Unicode al texto ASCII (https://stackoverflow.com/a/518232/2809427)\n",
        "def unicodeToAscii(s):\n",
        "  return ''.join(\n",
        "      c for c in unicodedata.normalize('NFD', s)\n",
        "      if unicodedata.category(c) != 'Mn'\n",
        "      and c in all_letters\n",
        "  )\n",
        "\n",
        "#print(unicodeToAscii('Ślusàrski'))\n",
        "\n",
        "# Leer los archivos y los cargaremos en un estructura de tipo diccionario\n",
        "category_lines = {}\n",
        "all_categories = []\n",
        "\n",
        "def readLines(filenames):\n",
        "  lines = open(filename, encoding='utf-8').read().strip().split('\\n')\n",
        "  return [unicodeToAscii(line) for line in lines]\n",
        "\n",
        "for filename in glob.glob(f'{PATH_TEXT}/*.txt'):\n",
        "  category = os.path.splitext(os.path.basename(filename))[0]\n",
        "  all_categories.append(category)\n",
        "  lines = readLines(filename)\n",
        "  category_lines[category] = lines\n",
        "\n",
        "n_categories = len(all_categories)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bLj5VD6iYfWN"
      },
      "source": [
        "Ahora tenemos `category_lines`, un diccionario que mapea cada categoría (idioma) a una lista de líneas (nombres). También llevamos un registro de `all_categories` (sólo una lista de idiomas) y `n_categories` para su posterior referencia."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J3hbZSkxY3GD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7413fa77-d29c-4573-d244-3b6104869f93"
      },
      "source": [
        "print(all_categories)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oryk6ERs1eQR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "outputId": "3cb7eead-9238-40ca-dd29-b51e0aba49c3"
      },
      "source": [
        "print(category_lines['Greek'][:5])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-482cba7e29f9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcategory_lines\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Greek'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m: 'Greek'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EozPr7wIY5Ub"
      },
      "source": [
        "## 2. Convirtiendo los nombres a tensores\n",
        "\n",
        "Ahora que tenemos todos los nombres organizados, necesitamos convertirlos en\n",
        "Tensores para hacer cualquier uso de ellos.\n",
        "\n",
        "Para representar una sola **letra**, usamos un \"***one-hot vector***\" de tamaño `<1 x n_letters>`. Un vector de una letra (***one-hot vector***), consiste en un arreglo que contiene ceros y un (uno solo) en el índice de la letra que se quiere representar. Por ejemplo, `\"b\"= <0 1 0 0 0 ...>`.\n",
        "\n",
        "Para representar una **palabra** unimos varios vectores de una letra en una matriz 2D `<line_length x 1 x n_letters>`.\n",
        "\n",
        "La dimesión 1 extra indica a Pytorch que esa estructura está en lotes (*batch*), indicando que el tamaño del lote es uno.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dtyEfy5qa18Q",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "outputId": "3ee4ac5c-08b5-49df-978b-c6990e4ce5bf"
      },
      "source": [
        "import torch\n",
        "\n",
        "# Encontrar el indice de una letra en all_letters\n",
        "def letterToIndex(letter):\n",
        "  return all_letters.find(letter)\n",
        "\n",
        "# Construir un vector one-hot letter (Ilustrativo)\n",
        "def letterToTensor(letter):\n",
        "  tensor = torch.zeros(1, n_letters)\n",
        "  tensor[0][letterToIndex(letter)] = 1\n",
        "  return tensor\n",
        "\n",
        "def lineToTensor(line):\n",
        "  tensor = torch.zeros(len(line), 1, n_letters)\n",
        "  for li, letter in enumerate(line):\n",
        "    tensor[li][0][letterToIndex(letter)] = 1\n",
        "\n",
        "  return tensor\n",
        "\n",
        "print(lineToTensor('Juan'))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0.]],\n",
            "\n",
            "        [[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0.]]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DexIKj84gf4i"
      },
      "source": [
        "## 3. Creando la red\n",
        "\n",
        "Antes del *autograd*, la creación de una red neural recurrente en Torch implicaba\n",
        "clonar los parámetros de una capa en varios pasos de tiempo. Las capas\n",
        "mantenían estados y gradientes ocultos que ahora son manejados enteramente por la librería. Esto significa que puedes implementar un RNN de una manera muy \"pura\", como capas regulares de alimentación...\n",
        "\n",
        "Este módulo RNN (en su mayoría copiado de \"The PyTorch for Torch users\")\n",
        "tutorial <https://pytorch.org/tutorials/beginner/former_torchies/\n",
        "nn_tutorial.html#example-2-recurrent-net>`__)\n",
        "son sólo 2 capas lineales que operan en un estado de entrada y oculto, con\n",
        "una capa de LogSoftmax después de la salida.\n",
        "\n",
        "Figura:: https://i.imgur.com/Z2xbySO.png\n",
        "\n",
        "Figura [(softmax)](https://cdn-images-1.medium.com/freeze/max/1000/1*ghDbeGZZUloPOJK7g3OyBg.png?q=20)::\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n2OHMKgOghYc"
      },
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class RNN(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, output_size):\n",
        "    super(RNN, self).__init__()\n",
        "\n",
        "    self.hidden_size = hidden_size\n",
        "\n",
        "    self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
        "    self.i2o = nn.Linear(input_size + hidden_size, output_size)\n",
        "    self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "  def forward(self, input, hidden):\n",
        "    combined = torch.cat((input, hidden), 1) # vector [1 x (input + hidden)]\n",
        "    hidden = self.i2h(combined)\n",
        "    output = self.i2o(combined)\n",
        "    output = self.softmax(output)\n",
        "    return output, hidden\n",
        "\n",
        "  def initHidden(self):\n",
        "    return torch.zeros(1, self.hidden_size)\n",
        "\n",
        "n_hidden = 128\n",
        "\n",
        "rnn = RNN(n_letters, n_hidden, n_categories)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nkxMoHhkjsrU"
      },
      "source": [
        "Para ejecutar un paso de esta red necesitamos pasar una entrada (en nuestro caso, el\n",
        "Tensor para la letra actual) y un estado oculto anterior (que nosotros\n",
        "inicializar como ceros al principio). Recuperaremos la salida (probabilidad de\n",
        "cada idioma) y un siguiente estado oculto (que guardamos para el próximo\n",
        "paso)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zWiTauhejr5d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "96663bd5-fd33-4cea-8198-6e13fc428101"
      },
      "source": [
        "input = letterToTensor('A')\n",
        "hidden = torch.zeros(1, n_hidden)\n",
        "\n",
        "output, next_hidden = rnn(input, hidden)\n",
        "\n",
        "print(output)\n",
        "print(next_hidden)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([], size=(1, 0), grad_fn=<LogSoftmaxBackward>)\n",
            "tensor([[-0.0583,  0.0683, -0.0833, -0.0089, -0.0476,  0.0513, -0.0464, -0.0823,\n",
            "         -0.0220, -0.0193, -0.0094,  0.0042,  0.0677,  0.0451,  0.0354,  0.0389,\n",
            "          0.0186, -0.0281,  0.0198,  0.1098,  0.0622, -0.1155,  0.0309, -0.0538,\n",
            "         -0.0121, -0.0709,  0.0273,  0.0013,  0.0385,  0.1035,  0.1086, -0.1184,\n",
            "         -0.0885,  0.0652,  0.0518,  0.0027, -0.0679, -0.0136, -0.0189, -0.0668,\n",
            "          0.0025,  0.0135,  0.0063,  0.0865, -0.0889,  0.0212,  0.0513,  0.0792,\n",
            "         -0.0682, -0.0669,  0.0393, -0.0203,  0.0263,  0.0314,  0.0502, -0.0559,\n",
            "          0.0417,  0.0831,  0.0327, -0.0156,  0.1209,  0.0232, -0.0541,  0.0897,\n",
            "          0.0967, -0.0185,  0.0928,  0.0251, -0.0788, -0.0258, -0.0359,  0.0519,\n",
            "         -0.0270,  0.1226, -0.1099, -0.1179, -0.0303,  0.0329, -0.0772, -0.0757,\n",
            "          0.1358, -0.1257, -0.0118,  0.1247,  0.1014,  0.0348, -0.0322, -0.0913,\n",
            "         -0.0320, -0.1063, -0.0172,  0.0094, -0.0095,  0.1086,  0.0079, -0.0174,\n",
            "          0.0256,  0.0879,  0.0077,  0.0759,  0.0486, -0.0572,  0.1020,  0.0290,\n",
            "          0.0892,  0.0034, -0.0053,  0.1314, -0.0053,  0.0174, -0.1195, -0.0732,\n",
            "         -0.0686,  0.0262, -0.0645,  0.0446,  0.0318,  0.0224,  0.0362,  0.0655,\n",
            "         -0.0735,  0.0586, -0.0187,  0.1003, -0.1264,  0.0397,  0.0553, -0.0925]],\n",
            "       grad_fn=<AddmmBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4sYVj4Z4kHqJ"
      },
      "source": [
        "Para mejorar la eficiencia no queremos crear un nuevo Tensor para\n",
        "cada paso, así que usaremos ``lineToTensor`` en lugar de\n",
        "``letterToTensor`` y usa *slices*. Esto podría ser optimizado aún más por\n",
        "precomputando lotes de Tensores."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jQfQchKfkX1J"
      },
      "source": [
        "input = lineToTensor('Alberto')\n",
        "hidden = torch.zeros(1, n_hidden)\n",
        "\n",
        "output, next_hidden = rnn(input[0], hidden)\n",
        "print(output)\n",
        "print(next_hidden)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DNHadH4skjwB"
      },
      "source": [
        "Como puedes ver, la salida es un tensor ``<1 x n_categories>``, donde\n",
        "cada *item* es la probabilidad de esa categoría (más alta es más probable).\n",
        "\n",
        "## 4. Entrenando la red\n",
        "\n",
        "Preparando los datos\n",
        "--------------------\n",
        "\n",
        "Antes de entrar en el entrenamiento debemos hacer algunas funciones de ayuda. La primera es interpretar el resultado de la red, que sabemos que es una probabilidad de cada categoría. Podemos usar Tensor.topk para obtener el índice de mayor valor:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3atWo1eXNl68"
      },
      "source": [
        "def categoryFromOutput(output):\n",
        "  top_n, top_i = output.topk(1)\n",
        "  category_i = top_i[0].item()\n",
        "  return all_categories[category_i], category_i\n",
        "\n",
        "print(categoryFromOutput(output))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OI1PH7pPNyVk"
      },
      "source": [
        "También queremos una forma rápida de obtener un ejemplo de entrenamiento (un nombre y su\n",
        "idioma):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9owXNOWBN3Cs"
      },
      "source": [
        "import random\n",
        "\n",
        "# Elige un elemento de lista en forma aleatoria\n",
        "def randomChoice(lista):\n",
        "  return lista[random.randint(0, len(lista)-1)]\n",
        "\n",
        "def randomTrainingExample():\n",
        "  category = randomChoice(all_categories)\n",
        "  # Selecciono la palabra (nombre) de una categoria\n",
        "  line = randomChoice(category_lines[category])\n",
        "  # Convertir tensores\n",
        "  category_tensor = torch.tensor([all_categories.index(category)], dtype=torch.long)\n",
        "  line_tensor = lineToTensor(line)\n",
        "  return category, line, category_tensor, line_tensor\n",
        "\n",
        "print(all_categories)\n",
        "for i in range(20):\n",
        "  category, line, category_tensor, line_tensor = randomTrainingExample()\n",
        "  print('categoria=', category, '/ palabra=', line)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2fN8ruQMOBtN"
      },
      "source": [
        "Entrenamiento de la red\n",
        "--------------------\n",
        "\n",
        "Ahora todo lo que se necesita para entrenar a esta red es mostrarle un montón de ejemplos.\n",
        "\n",
        "Para la función de pérdida `n.NLLLoss` es apropiado, ya que la última\n",
        "La capa del RNN es `nn.LogSoftmax`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZRP2IuHrOSTQ"
      },
      "source": [
        "criterion = nn.NLLLoss()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J6zoXfiPOW7s"
      },
      "source": [
        "Cada ciclo de entrenamiento hará:\n",
        "\n",
        "1. Crear tensores de entrada y de destino\n",
        "\n",
        "2. Crear un estado inicial oculto igual a cero\n",
        "\n",
        "3. Leer cada letra y mantener el estado oculto para la próxima letra.\n",
        "\n",
        "4. Comparar el resultado final con el objetivo\n",
        "\n",
        "5. *Back-propagate*\n",
        "\n",
        "6. Devolver la salida y la pérdida"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FdPmWvvoPHpV"
      },
      "source": [
        "learning_rate = 0.005\n",
        "\n",
        "def train(category_tensor, line_tensor):\n",
        "  # Inicializar la capa hidden en cero\n",
        "  hidden = rnn.initHidden()\n",
        "\n",
        "  rnn.zero_grad()\n",
        "\n",
        "  for i in range(line_tensor.size()[0]):\n",
        "    output, hidden = rnn(line_tensor[i], hidden)\n",
        "\n",
        "  loss = criterion(output, category_tensor)\n",
        "  loss.backward()\n",
        "\n",
        "  # Actualizar los parametros del modelo\n",
        "  for p in rnn.parameters():\n",
        "    p.data.add_(p.grad.data, alpha=-learning_rate)\n",
        "\n",
        "  return output, loss.item()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x8idXbiLPMCt"
      },
      "source": [
        "Ahora sólo tenemos que correr esto con un montón de ejemplos. Como la función del entrenamiento devuelve tanto la salida como la pérdida, podemos imprimir sus predicciones y también llevar la cuenta de la pérdida. Ya que hay miles de ejemplos, imprimimos sólo cada uno de ellos y tomamos un promedio de la pérdida."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hsSnEVMnPmjF"
      },
      "source": [
        "import time\n",
        "import math\n",
        "\n",
        "n_iters = 100000\n",
        "print_every = 5000\n",
        "plot_every = 1000\n",
        "\n",
        "# Guardamos la perdida\n",
        "current_loss = 0\n",
        "all_losses = []\n",
        "\n",
        "def timeSince(since):\n",
        "  now = time.time()\n",
        "  s = now -since\n",
        "  m = math.floor(s/60)\n",
        "  s -= m *60\n",
        "  return '%dm %ds' % (m, s)\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "for iter in range(1, n_iters + 1):\n",
        "  # Obtener una categoria y una palabra de forma aleatoria\n",
        "  category, line, category_tensor, line_tensor = randomTrainingExample()\n",
        "  output, loss = train(category_tensor, line_tensor)\n",
        "  current_loss += loss\n",
        "\n",
        "  # Imprimir los resultados\n",
        "  if iter % print_every == 0: \n",
        "    guess, guess_i = categoryFromOutput(output)\n",
        "    correct = 'OK' if guess == category else 'ERROR (%s)' % category\n",
        "    print('%d %d%% (%s) %.4f %s / %s %s' % (iter, iter/n_iter * 100, timeSince(start),loss, line, guess, correct))\n",
        "\n",
        "  if iter % plot_every == 0:\n",
        "    all_losses.append(current_loss / plot_every)\n",
        "    current_loss = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TkGnsConPpG1"
      },
      "source": [
        "## 5. Graficando los resultados\n",
        "\n",
        "Graficar la pérdida histórica de `all_loses` muestra el aprendizaje de la red."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "okJbnsOeP8S2"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(all_losses)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ghIyV3qMQBul"
      },
      "source": [
        "# 6. Evaluando los resultados\n",
        "\n",
        "Para ver cómo funciona la red en diferentes categorías, vamos a\n",
        "crear una **matriz de confusión**, indicando para cada idioma real (filas)\n",
        "qué idioma adivina la red (columnas). Para calcular la confusión\n",
        "matriz un montón de muestras se ejecutan a través de la red con\n",
        "`Evaluar`, que es lo mismo que `Entrenar`, menos el respaldo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nvw2_qg-QY3G"
      },
      "source": [
        "# Keep track of correct guesses in a confusion matrix\n",
        "confusion = torch.zeros(n_categories, n_categories)\n",
        "n_confusion = 10000\n",
        "\n",
        "# Just return an output given a line\n",
        "def evaluate(line_tensor):\n",
        "    hidden = rnn.initHidden()\n",
        "\n",
        "    for i in range(line_tensor.size()[0]):\n",
        "        output, hidden = rnn(line_tensor[i], hidden)\n",
        "\n",
        "    return output\n",
        "\n",
        "# Go through a bunch of examples and record which are correctly guessed\n",
        "for i in range(n_confusion):\n",
        "    category, line, category_tensor, line_tensor = randomTrainingExample()\n",
        "    output = evaluate(line_tensor)\n",
        "    guess, guess_i = categoryFromOutput(output)\n",
        "    category_i = all_categories.index(category)\n",
        "    confusion[category_i][guess_i] += 1\n",
        "\n",
        "# Normalize by dividing every row by its sum\n",
        "for i in range(n_categories):\n",
        "    confusion[i] = confusion[i] / confusion[i].sum()\n",
        "\n",
        "# Set up plot\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111)\n",
        "cax = ax.matshow(confusion.numpy())\n",
        "fig.colorbar(cax)\n",
        "\n",
        "# Set up axes\n",
        "ax.set_xticklabels([''] + all_categories, rotation=90)\n",
        "ax.set_yticklabels([''] + all_categories)\n",
        "\n",
        "# Force label at every tick\n",
        "ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "\n",
        "# sphinx_gallery_thumbnail_number = 2\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "676JMowHQdNN"
      },
      "source": [
        "Puedes elegir los puntos más brillantes en el eje principal que muestren qué\n",
        "idiomas que adivina incorrectamente, por ejemplo, el chino para el coreano, y el español para el italiano. Parece que le va muy bien con el griego, y muy mal con Inglés (tal vez debido a la superposición con otros idiomas)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HkdPiWitQugn"
      },
      "source": [
        "def predict(input_line, n_predictions=3):\n",
        "    print('\\n> %s' % input_line)\n",
        "    with torch.no_grad():\n",
        "        output = evaluate(lineToTensor(input_line))\n",
        "\n",
        "        # Get top N categories\n",
        "        topv, topi = output.topk(n_predictions, 1, True)\n",
        "        predictions = []\n",
        "\n",
        "        for i in range(n_predictions):\n",
        "            value = topv[0][i].item()\n",
        "            category_index = topi[0][i].item()\n",
        "            print('(%.2f) %s' % (value, all_categories[category_index]))\n",
        "            predictions.append([value, all_categories[category_index]])\n",
        "\n",
        "predict('Dovesky')\n",
        "predict('Jackson')\n",
        "predict('Satoshi')\n",
        "predict('Bekios')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dXkRhsgbQ98e"
      },
      "source": [
        "La versión completa del código la pueden revisar en <https://github.com/spro/practical-pytorch/tree/master/char-rnn-classification>`"
      ]
    }
  ]
}